{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6aa912",
   "metadata": {},
   "source": [
    "### Обучение пайплайна\n",
    "\n",
    "1. Загрузим данные https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
    "2. Соберем пайплайн с простейшим препроцессингом (tfidf) на текстовых данных\n",
    "3. Обучим логистическую регрессию и сохраним на диск предобученный пайплайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d777343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ba8c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145457</th>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>5.4</td>\n",
       "      <td>26.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>37.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>1016.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.5</td>\n",
       "      <td>26.1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145458</th>\n",
       "      <td>2017-06-24</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>7.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SE</td>\n",
       "      <td>28.0</td>\n",
       "      <td>SSE</td>\n",
       "      <td>...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1019.4</td>\n",
       "      <td>1016.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145459</th>\n",
       "      <td>2017-06-25</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>14.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ESE</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1020.2</td>\n",
       "      <td>1017.9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  \\\n",
       "145457  2017-06-23    Uluru      5.4     26.9       0.0          NaN   \n",
       "145458  2017-06-24    Uluru      7.8     27.0       0.0          NaN   \n",
       "145459  2017-06-25    Uluru     14.9      NaN       0.0          NaN   \n",
       "\n",
       "        Sunshine WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  \\\n",
       "145457       NaN           N           37.0         SE  ...        53.0   \n",
       "145458       NaN          SE           28.0        SSE  ...        51.0   \n",
       "145459       NaN         NaN            NaN        ESE  ...        62.0   \n",
       "\n",
       "        Humidity3pm  Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  \\\n",
       "145457         24.0       1021.0       1016.8       NaN       NaN     12.5   \n",
       "145458         24.0       1019.4       1016.5       3.0       2.0     15.1   \n",
       "145459         36.0       1020.2       1017.9       8.0       8.0     15.0   \n",
       "\n",
       "        Temp3pm  RainToday  RainTomorrow  \n",
       "145457     26.1         No            No  \n",
       "145458     26.0         No            No  \n",
       "145459     20.9         No           NaN  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим данные\n",
    "df = pd.read_csv('weatherAUS.csv')\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e282aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные на train/test с предсказанной целевой переменной\n",
    "# и без нее и сохраним тестовую выборку на диск\n",
    "df_train = df[~df['RainTomorrow'].isna()]\n",
    "df_test = df[df['RainTomorrow'].isna()]\n",
    "\n",
    "#save df_test\n",
    "df_test.to_csv(\"df_test.csv\", index=None)\n",
    "\n",
    "# Разделим train_данные на train/test и сохраним тестовую выборку на диск\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train.drop('RainTomorrow', axis=1), \n",
    "    df_train['RainTomorrow'], \n",
    "    test_size=0.25,\n",
    "    stratify = df_train['RainTomorrow'],\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "#save test\n",
    "X_test.to_csv(\"X_test.csv\", index=None)\n",
    "y_test.to_csv(\"y_test.csv\", index=None)\n",
    "\n",
    "#save train\n",
    "X_train.to_csv(\"X_train.csv\", index=None)\n",
    "y_train.to_csv(\"y_train.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f32f8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# С помощью google.maps выпишим геоданные метеостанций\n",
    "stations = {\n",
    "    'Albury': [-36.065983, 146.933757],\n",
    "    'BadgerysCreek': [-33.874789, 150.742198],\n",
    "    'Cobar': [-31.498089, 145.840004],\n",
    "    'CoffsHarbour': [-30.302664, 153.121074],\n",
    "    'Moree': [-29.466090, 149.841702],\n",
    "    'Newcastle': [-32.928553, 151.780644],\n",
    "    'NorahHead': [-33.282690, 151.565798],\n",
    "    'NorfolkIsland': [-29.045021, 167.946025],\n",
    "    'Penrith': [-23.108893, 143.611796],\n",
    "    'Richmond': [-20.732187, 143.143603],\n",
    "    'Sydney': [-33.865255, 151.216484],\n",
    "    'SydneyAirport': [-33.939820, 151.178658],\n",
    "    'WaggaWagga': [-35.130577, 147.369054],\n",
    "    'Williamtown': [-32.795088, 151.853003],\n",
    "    'Wollongong': [-34.390606, 150.879616],\n",
    "    'Canberra': [-35.306904, 149.125529],\n",
    "    'Tuggeranong': [-35.407187, 149.102398],\n",
    "    'MountGinini': [-35.533333, 148.783333],\n",
    "    'Ballarat': [-37.558485, 143.855087],\n",
    "    'Bendigo': [-36.753422, 144.279909],\n",
    "    'Sale': [-38.103693, 147.068100],\n",
    "    'MelbourneAirport': [-37.673333, 144.843333],\n",
    "    'Melbourne': [-37.813747, 144.963005],\n",
    "    'Nhil': [-36.332988, 141.650199],    \n",
    "    'Mildura': [-37.558485, 143.855087],\n",
    "    'Nhil': [-36.332988, 141.650199],\n",
    "    'Portland': [-38.358293, 141.609900],\n",
    "    'Watsonia': [-37.711887, 145.083003],\n",
    "    'Dartmoor': [-37.927392, 141.276500],\n",
    "    'Brisbane': [-27.468545, 153.024029],\n",
    "    'Cairns': [-16.922018, 145.776439],\n",
    "    'GoldCoast': [-28.001431, 153.363484],\n",
    "    'Townsville': [-19.252226, 146.813347],\n",
    "    'Adelaide': [-34.951652, 138.593834],\n",
    "    'MountGambier': [-37.752367, 140.786152],\n",
    "    'Nuriootpa': [-34.487174, 138.975699],\n",
    "    'Woomera': [-37.398593, 140.335999],\n",
    "    'Albany': [-35.009677, 117.884181],\n",
    "    'Witchcliffe': [-34.025494, 115.100904],\n",
    "    'PearceRAAF': [-31.667778, 116.015],\n",
    "    'PerthAirport': [-31.940326, 115.966769],    \n",
    "    'Perth': [-32.088410, 115.895122],\n",
    "    'SalmonGums': [-32.98, 121.645],\n",
    "    'Walpole': [-34.975391, 116.734796],\n",
    "    'Hobart': [-42.859536, 147.309693],\n",
    "    'Launceston': [-41.446690, 147.139202],    \n",
    "    'AliceSprings': [-23.723467, 133.883826],\n",
    "    'Darwin': [-12.465673, 130.842738],\n",
    "    'Katherine': [-14.465293, 132.263199],\n",
    "    'Uluru': [-25.345400, 131.033856]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d9b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, stations=stations):\n",
    "    \n",
    "    def search_nearest_station(first_station, max_distance):\n",
    "        \"\"\"\n",
    "        Осуществляет поиск ближайших стаций в заданном радиусе.\n",
    "        На выходе отсорированный по удаленности список станций.\n",
    "        \"\"\"\n",
    "        new_stations = stations.copy() # создадим корию словоря со станциями\n",
    "        distance_dict = {} # Создадим пустой словарь для расстояний между станциями\n",
    "        x1, y1 = stations[first_station] # координаты первой станции\n",
    "        new_stations.pop(first_station) # удалим ее из копии словоря со станциями\n",
    "        for station in new_stations: # для каждой стации \n",
    "            x2, y2 = new_stations[station] # определим ее координаты\n",
    "            # и измерием расстояние до первой станции \n",
    "            distance = np.math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
    "            # добавим станцию и расстояние в словарь \n",
    "            distance_dict[station] = round(distance, 3)\n",
    "        # сформируем отсорированный по расстояниям список станций находящихся в \n",
    "        # пределах заданного радиуса\n",
    "        new_stations = np.array([key for key, value in sorted(distance_dict.items(), key=lambda item: item[1])\n",
    "                                 if value < max_distance])\n",
    "        return new_stations\n",
    "    \n",
    "    def my_fillna(i, j):\n",
    "        \"\"\"\n",
    "        Ищет значение на ближейшей станции\n",
    "        \"\"\"\n",
    "        value = df.iloc[i, j] # координаты значения\n",
    "        date, location = df.iloc[i, :2] # даты и место измерения\n",
    "        stations_list = stations_distance_list[int(location), 1] # список ближайших станций\n",
    "        while stations_list.shape[0]: # если станции есть\n",
    "            # получаем значение искомого параметра с соседней станции в этот день\n",
    "            new_value = df.loc[(df['Date'] == date) & (df['Location'] == stations_list[0]), df.columns[j]]\n",
    "            if len(new_value) and not np.isnan(new_value.iloc[0]): # если найдено значение\n",
    "                value = new_value.iloc[0] # меняем его на новое\n",
    "                break # выходим из цикла\n",
    "            # если значение не найдено, проверяем следующую станцию\n",
    "            else: stations_list = stations_list[1:]  \n",
    "        return value\n",
    "    \n",
    "    # Заменим значения RainToday на 0 и 1:\n",
    "    binary_to_numbers = {'No': 0, 'Yes': 1}\n",
    "    df['RainToday'] = df['RainToday'].replace(binary_to_numbers)\n",
    "    \n",
    "    # Преобразуем значение даты в количество дней с 1970-01-01\n",
    "    df['Date'] = (pd.to_datetime(df['Date'])- pd.Timestamp('1970-01-01')) // pd.Timedelta('1days')\n",
    "    \n",
    "    # Создадим датафрем с наименованиями станций и их координатами\n",
    "    sdf = pd.DataFrame({'Location': stations.keys(), 'coordinate': stations.values()})\n",
    "    sdf['longitude'] = sdf['coordinate'].apply(lambda x: x[0])\n",
    "    sdf['latitude'] = sdf['coordinate'].apply(lambda x: x[1])\n",
    "    sdf.drop('coordinate', axis=1, inplace=True)\n",
    "    \n",
    "    # Добавим координаты станций в исследуемый data frame\n",
    "    df = df.merge(sdf, on=['Location'], how='left')\n",
    "\n",
    "    # Присвоим станциям номера \n",
    "    locations = {name: i for i, name in enumerate(stations)}\n",
    "\n",
    "    # Заменим их наименования на номера\n",
    "    df['Location'] = df['Location'].replace(locations)\n",
    "\n",
    "    # Сформируем корректный data frame\n",
    "    columns = list(df.columns[:2]) + list(df.columns[-2:]) + list(df.columns[2:-2])\n",
    "    df = df[columns]\n",
    "    df.head()\n",
    "    \n",
    "    # Оставшиеся категориальные переменные представляют собой направление верта\n",
    "    # Заменим их на пoрядковые номера значений их азимутов\n",
    "    categorical_columns = df.select_dtypes(include='object').columns.tolist()\n",
    "    cardinal_points = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "    cardinal_points_dict = {i: cardinal_points.index(i) for i in cardinal_points}\n",
    "    for col in categorical_columns: df[col] = df[col].replace(cardinal_points_dict)\n",
    "        \n",
    "    # Сформируем списоки соседних стаций для каждой стации в радиусе 2 градуса\n",
    "    max_distance = 2\n",
    "    stations_distance_list = np.array([[location, search_nearest_station(location, max_distance)] for location in locations])\n",
    "    \n",
    "    # Заполним пропуски значениями параметров на ближайших стациях\n",
    "    for i in tqdm(np.arange(df.shape[0])): # по строкам\n",
    "        for j in np.arange(df.shape[1]): # по столбцам\n",
    "            if np.isnan(df.iloc[i, j]): # если не np.NaN \n",
    "                df.iloc[i, j] = my_fillna(i, j) # заменяем на значение с ближайшей станции\n",
    "                                                # если конечно стация есть\n",
    "                                                # и если есть значение\n",
    "                        \n",
    "    # Оставшиеся пропуски заменим на медианные значения в месте измерения\n",
    "    for col in df.columns:\n",
    "        wdf = pd.DataFrame(df.groupby(['Location'])[col].median().rename(f'{col}ByLocation')).reset_index()\n",
    "        df = df.merge(wdf, on=['Location'], how='left')\n",
    "        df[col].fillna(df[f'{col}ByLocation'], inplace=True)\n",
    "        df.drop(f'{col}ByLocation', axis=1, inplace=True)\n",
    "    \n",
    "    # Если пропуски остались\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    # Для построения универсального пайплайна обработки вернем обратно типы данных\n",
    "    \n",
    "    # Date\n",
    "    df['Date'] = df['Date'].apply(lambda x: datetime.fromordinal(x+719163).strftime('%Y-%m-%d'))\n",
    "    # Location\n",
    "    df['Location'] = df['Location'].replace({i: name for i, name in enumerate(stations)})\n",
    "    # WindGustDir, WindDir9am, WindDir3pm\n",
    "    for column in ['WindGustDir', 'WindDir9am', 'WindDir3pm']:\n",
    "        df[column] = df[column].replace({i: name for i, name in enumerate(cardinal_points)})\n",
    "    # RainToday\n",
    "    df['RainToday'] = df['RainToday'].replace({0: 'No', 1: 'Yes'})\n",
    "    \n",
    "    # Удалим вспомогательные столбцы longitude, latitude\n",
    "    df.drop(['longitude', 'latitude'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba3372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_preprocessing(y_train):\n",
    "    # заменим значения целевой переменной на 0 и 1:\n",
    "    return y_train.replace({'No': 0, 'Yes': 1})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5810690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Соберем pipeline\n",
    "class DateSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[self.key]  = (pd.to_datetime(X[self.key])- pd.Timestamp('1970-01-01')) // pd.Timedelta('1days')\n",
    "        return X[[self.key]]\n",
    "\n",
    "class LocationSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.location = stations.copy()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        sdf = pd.DataFrame({'Location': self.location.keys(), 'Coordinate': self.location.values()})\n",
    "        sdf['Longitude'] = sdf['Coordinate'].apply(lambda x: x[0])\n",
    "        sdf['Latitude'] = sdf['Coordinate'].apply(lambda x: x[1])\n",
    "        sdf.drop('Coordinate', axis=1, inplace=True)\n",
    "        X = X.merge(sdf, on=[self.key], how='left')\n",
    "        \n",
    "        return X[['Longitude', 'Latitude']]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "class WindDirSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.cardinal_points = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', \n",
    "                                'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[self.key] = X[self.key].replace({name: i for i, name in enumerate(self.cardinal_points)})\n",
    "        return X[[self.key]]\n",
    "    \n",
    "class BinarySelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[self.key] = X[self.key].replace({'No': 0, 'Yes': 1}) \n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c60e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Q\\AppData\\Local\\Temp/ipykernel_1904/1550767922.py:77: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  stations_distance_list = np.array([[location, search_nearest_station(location, max_distance)] for location in locations])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3b528434b94d89803984cda51936e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Предобработка данных\n",
    "    X_train = data_preprocessing(X_train)\n",
    "    y_train = target_preprocessing(y_train)\n",
    "\n",
    "    # Зададим списки признаков\n",
    "    date_columns = ['Date']\n",
    "    location_columns = ['Location']\n",
    "    continuous_columns = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine','WindGustSpeed', \n",
    "                          'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', \n",
    "                          'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']\n",
    "    wind_dir_columns = ['WindGustDir', 'WindDir9am', 'WindDir3pm']\n",
    "    binary_columns = ['RainToday']\n",
    "\n",
    "    # Под каждый признак создадим трансформер и объединить их в список\n",
    "    final_transformers = list()\n",
    "    \n",
    "    for date_col in date_columns:\n",
    "        date_transformer = Pipeline([\n",
    "                ('selector', DateSelector(key=date_col))\n",
    "                ])    \n",
    "        final_transformers.append((date_col, date_transformer))\n",
    "    \n",
    "    for location_col in location_columns:\n",
    "        location_transformer = Pipeline([\n",
    "                ('selector', LocationSelector(key=location_col))\n",
    "                ])\n",
    "        final_transformers.append((location_col, location_transformer))\n",
    "    \n",
    "    for cont_col in continuous_columns:\n",
    "        cont_transformer = Pipeline([\n",
    "                ('selector', NumberSelector(key=cont_col))\n",
    "                ])\n",
    "        final_transformers.append((cont_col, cont_transformer))\n",
    "    \n",
    "    for wind_dir_col in wind_dir_columns:\n",
    "        wind_dir_transformer = Pipeline([\n",
    "                ('selector', WindDirSelector(key=wind_dir_col))\n",
    "                ])\n",
    "        final_transformers.append((wind_dir_col, wind_dir_transformer))\n",
    "    \n",
    "    for binary_col in binary_columns:\n",
    "        binary_transformer = Pipeline([\n",
    "                ('selector', BinarySelector(key=binary_col))\n",
    "                ])\n",
    "        final_transformers.append((binary_col, binary_transformer))\n",
    "\n",
    "    # Объединим все это в единый пайплайн\n",
    "    feats = FeatureUnion(final_transformers)\n",
    "    feature_processing = Pipeline([('feats', feats)])\n",
    "\n",
    "    # Добавим модель\n",
    "    pipeline = Pipeline([\n",
    "        ('features', feats),\n",
    "        ('classifier', GradientBoostingClassifier(\n",
    "            n_estimators=700,\n",
    "            min_samples_leaf=7,\n",
    "            max_depth=10\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Обучим пайплайны\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Сохраним модель\n",
    "    with open(\"gbc_pipeline.dill\", \"wb\") as f:\n",
    "        dill.dump(pipeline, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
